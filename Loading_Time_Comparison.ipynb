{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string \n",
    "import random \n",
    "import time\n",
    "import dask.dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1585883810.618082"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# small data\n",
    "\n",
    "## 00_Generating a CSV file with random data ~75 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting the number of rows for the CSV file (1M)\n",
    "N = 1000000\n",
    "\n",
    "# creating a pandas dataframe (df) with 8 columns and N rows with random integers between 999 and 999999 and with column names from A to H\n",
    "df = pd.DataFrame(\n",
    "    np.random.randint(9,999, size=(N, 7)), \n",
    "    columns=list('ABCDEFG'))\n",
    "\n",
    "# creating one column 'H' of float type using the uniform distribution\n",
    "df['H'] = np.random.rand(N)\n",
    "\n",
    "# creating two additional columns with random strings\n",
    "df['I'] = pd.util.testing.rands_array(10, N)\n",
    "df['J'] = pd.util.testing.rands_array(10, N)\n",
    "\n",
    "# expect: 1M rows x 10 columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the dataframe to csv ~75MB\n",
    "df.to_csv(\"test_data_75MB.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01_pandas_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.310065746307373 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "df = pd.read_csv(\"test_data_75MB.csv\") \n",
    "\n",
    "print(\"%s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02_pandas_with_chunsize_option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.338909864425659 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "df_chunk = pd.read_csv(\"test_data_75MB.csv\", chunksize=5000) \n",
    "chunk_list = []  \n",
    "\n",
    "for chunk in df_chunk:  \n",
    "    chunk_list.append(chunk)\n",
    "df_concat = pd.concat(chunk_list)\n",
    "\n",
    "print(\"%s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03_dask_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3885600566864014 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "data = dask.dataframe.read_csv(\"test_data_75MB.csv\").compute()\n",
    "\n",
    "print(\"%s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, unlike pandas with dask the data is not fully loaded into memory, but is ready to be processed. Also certain opperations can be performed again without loading the whole dataset into memory. Another advantage is that the most functions used with pandas can be also use with dask. The differences arise from the parallel nature of dask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# medium data\n",
    "\n",
    "## 00_Generating a CSV file with random data ~75 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000000, 10)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting the number of rows for the CSV file (10M)\n",
    "N = 10000000\n",
    "\n",
    "# creating a pandas dataframe (df) with 8 columns and N rows with random integers between 999 and 999999 and with column names from A to H\n",
    "df = pd.DataFrame(\n",
    "    np.random.randint(9,999, size=(N, 7)), \n",
    "    columns=list('ABCDEFG'))\n",
    "\n",
    "# creating one column 'H' of float type using the uniform distribution\n",
    "df['H'] = np.random.rand(N)\n",
    "\n",
    "# creating two additional columns with random strings\n",
    "df['I'] = pd.util.testing.rands_array(10, N)\n",
    "df['J'] = pd.util.testing.rands_array(10, N)\n",
    "\n",
    "# expect: 10M rows x 10 columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the dataframe to csv ~75MB\n",
    "df.to_csv(\"test_data_750MB.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "data_analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
