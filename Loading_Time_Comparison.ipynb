{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expirementing data loading time with Pandas vs. Dask\n",
    "\n",
    "File size for comparison:\n",
    "- Small size file: 75MB\n",
    "- Medium size file: 1.8GB\n",
    "- Large size file: 12.7GB\n",
    "\n",
    "Who is faster at what condition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe\n",
    "import multiprocessing as mp\n",
    "import psutil\n",
    "import string \n",
    "import random \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1585927178.1756659"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# small data\n",
    "\n",
    "## 00_Generating a CSV file with random data ~75 MB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting the number of rows for the CSV file (1M)\n",
    "N = 1000000\n",
    "\n",
    "# creating a pandas dataframe (df) with 7 columns with column names from A to G\n",
    "df = pd.DataFrame(\n",
    "    np.random.randint(9,999, size=(N, 7)), \n",
    "    columns=list('ABCDEFG'))\n",
    "\n",
    "# creating one column 'H' of float type using the uniform distribution\n",
    "df['H'] = np.random.rand(N)\n",
    "\n",
    "# creating two additional columns with random strings\n",
    "df['I'] = pd.util.testing.rands_array(10, N)\n",
    "df['J'] = pd.util.testing.rands_array(10, N)\n",
    "\n",
    "# expect: 1M rows x 10 columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the dataframe to csv ~75MB\n",
    "df.to_csv(\"test_data_75MB.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01_pandas_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2899370193481445 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "data = pd.read_csv(\"test_data_75MB.csv\") \n",
    "\n",
    "print(\"%s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02_pandas_with_chunsize_option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4829139709472656 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "df_chunk = pd.read_csv(\"test_data_75MB.csv\", chunksize=100000) # 10% of total rows#\n",
    "chunk_list = []  \n",
    "\n",
    "for chunk in df_chunk:  \n",
    "    chunk_list.append(chunk)\n",
    "data = pd.concat(chunk_list)\n",
    "\n",
    "print(\"%s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03_dask_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3889448642730713 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "data = dask.dataframe.read_csv(\"test_data_75MB.csv\").compute()\n",
    "\n",
    "print(\"%s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With small file loading, read_csv without chunsize is ideal. Adding chunsize option actually make it less efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# medium data\n",
    "\n",
    "## 00_Generating a CSV file with random data ~2 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000000, 15)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting the number of rows for the CSV file (10M)\n",
    "N = 10000000\n",
    "\n",
    "# creating a pandas dataframe (df) with 8 columns and N rows with random integers between 999 and 999999 and with column names from A to H\n",
    "df = pd.DataFrame(\n",
    "    np.random.randint(9,999, size=(N, 7)), \n",
    "    columns=list('ABCDEFG'))\n",
    "\n",
    "# creating one column 'H' of float type using the uniform distribution\n",
    "df['H'] = np.random.rand(N)\n",
    "\n",
    "# creating two additional columns with random strings\n",
    "df['I'] = pd.util.testing.rands_array(10, N)\n",
    "df['J'] = pd.util.testing.rands_array(10, N)\n",
    "df['K'] = pd.util.testing.rands_array(10, N)\n",
    "df['L'] = pd.util.testing.rands_array(10, N)\n",
    "df['M'] = pd.util.testing.rands_array(10, N)\n",
    "df['N'] = pd.util.testing.rands_array(10, N)\n",
    "df['O'] = pd.util.testing.rands_array(10, N)\n",
    "\n",
    "# expect: 10M rows x 15 columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the dataframe to csv 1.8GB\n",
    "df.to_csv(\"test_data_1GB.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01_pandas_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.06393694877625 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "df = pd.read_csv(\"test_data_1GB.csv\") \n",
    "\n",
    "print(\"%s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02_pandas_with_chunsize_option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.94773411750793 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "df_chunk = pd.read_csv(\"test_data_1GB.csv\", chunksize=100000) \n",
    "chunk_list = []  \n",
    "\n",
    "for chunk in df_chunk:  \n",
    "    chunk_list.append(chunk)\n",
    "df_concat = pd.concat(chunk_list)\n",
    "\n",
    "print(\"%s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03_dask_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.132309913635254 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "data = dask.dataframe.read_csv(\"test_data_1GB.csv\").compute()\n",
    "\n",
    "print(\"%s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When file size gets to ~2GB, doing something more than `read_csv` shows significant benefit. Dask is slightly faster than pandas with chunksize, but similar performance.\n",
    "\n",
    "Most functions used with pandas can be also use with dask. The differences arise from the parallel nature of dask.\n",
    "\n",
    "Unlike pandas, with dask the data is not fully loaded into memory but is ready to be processed. Certain opperations can be performed again without loading the whole dataset into memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# large data\n",
    "\n",
    "## 00_Generating a CSV file with random data ~10 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000000, 25)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting the number of rows for the CSV file (50M)\n",
    "N = 50000000\n",
    "\n",
    "# creating a pandas dataframe (df) with 10 columns and N rows with random between 99 and 9999 \n",
    "df = pd.DataFrame(\n",
    "    np.random.randint(99,9999, size=(N, 10)), \n",
    "    columns=list('ABCDEFGXYZ'))\n",
    "\n",
    "# creating one column 'H' of float type using the uniform distribution\n",
    "df['H1'] = np.random.rand(N)\n",
    "df['H2'] = np.random.rand(N)\n",
    "df['H3'] = np.random.rand(N)\n",
    "df['H4'] = np.random.rand(N)\n",
    "df['H5'] = np.random.rand(N)\n",
    "\n",
    "# creating additional columns with random strings\n",
    "df['K'] = pd.util.testing.rands_array(10, N)\n",
    "df['L'] = pd.util.testing.rands_array(10, N)\n",
    "df['M'] = pd.util.testing.rands_array(10, N)\n",
    "df['N'] = pd.util.testing.rands_array(10, N)\n",
    "df['O'] = pd.util.testing.rands_array(10, N)\n",
    "\n",
    "df['P'] = pd.util.testing.rands_array(10, N)\n",
    "df['Q'] = pd.util.testing.rands_array(10, N)\n",
    "df['R'] = pd.util.testing.rands_array(10, N)\n",
    "df['S'] = pd.util.testing.rands_array(10, N)\n",
    "df['T'] = pd.util.testing.rands_array(10, N)\n",
    "\n",
    "# expect: 50M rows x 25 columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the dataframe to csv 12.7GB\n",
    "df.to_csv(\"test_data_10GB.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01_pandas_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "572.7035613059998 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "data = dask.dataframe.read_csv(\"test_data_10GB.csv\").compute()\n",
    "\n",
    "print(\"%s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02_pandas_with_chunsize_option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "471.1917221546173 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "df_chunk = pd.read_csv(\"test_data_10GB.csv\", chunksize=100000) \n",
    "chunk_list = []  \n",
    "\n",
    "for chunk in df_chunk:  \n",
    "    chunk_list.append(chunk)\n",
    "data = pd.concat(chunk_list)\n",
    "\n",
    "print(\"%s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03_dask_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "787.500736951828 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "data = dask.dataframe.read_csv(\"test_data_10GB.csv\").compute()\n",
    "\n",
    "print(\"%s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunsize\n",
    "Regarding chunksize:\n",
    "https://stackoverflow.com/questions/35235010/what-is-the-optimal-chunksize-in-pandas-read-csv-to-maximize-speed\n",
    "\n",
    "There is no \"optimal chunksize\". Because chunksize only tells you the number of rows per chunk, not the memory-size of a single row, hence it's meaningless to try to make a rule-of-thumb on that. ([*] although generally I've only ever seen chunksizes in the range 100..64K)\n",
    "\n",
    "To get memory size, you'd have to convert that to a memory-size-per-chunk or -per-row...\n",
    "\n",
    "by looking at your number of columns, their dtypes, and the size of each; use either df.describe(), or else for more in-depth memory usage, by column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2147483648.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of cores and memory usage\n",
    "cpu = psutil.cpu_count()\n",
    "memory = psutil.virtual_memory().total\n",
    "memory/cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment:\n",
    "- 1k\n",
    "- 10k\n",
    "- 100k\n",
    "- 1m\n",
    "- 5m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small Data: 75 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.595816135406494 seconds\n"
     ]
    }
   ],
   "source": [
    "# 1K\n",
    "start_time = time.time()\n",
    "\n",
    "df_chunk = pd.read_csv(\"test_data_75MB.csv\", chunksize=1000) \n",
    "chunk_list = []  \n",
    "\n",
    "for chunk in df_chunk:  \n",
    "    chunk_list.append(chunk)\n",
    "data = pd.concat(chunk_list)\n",
    "\n",
    "print(\"%s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9769558906555176 seconds\n"
     ]
    }
   ],
   "source": [
    "# 10K\n",
    "start_time = time.time()\n",
    "\n",
    "df_chunk = pd.read_csv(\"test_data_75MB.csv\", chunksize=10000) \n",
    "chunk_list = []  \n",
    "\n",
    "for chunk in df_chunk:  \n",
    "    chunk_list.append(chunk)\n",
    "data = pd.concat(chunk_list)\n",
    "\n",
    "print(\"%s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4228427410125732 seconds\n"
     ]
    }
   ],
   "source": [
    "# 100K\n",
    "start_time = time.time()\n",
    "\n",
    "df_chunk = pd.read_csv(\"test_data_75MB.csv\", chunksize=100000) \n",
    "chunk_list = []  \n",
    "\n",
    "for chunk in df_chunk:  \n",
    "    chunk_list.append(chunk)\n",
    "data = pd.concat(chunk_list)\n",
    "\n",
    "print(\"%s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4417939186096191 seconds\n"
     ]
    }
   ],
   "source": [
    "# 1M\n",
    "start_time = time.time()\n",
    "\n",
    "df_chunk = pd.read_csv(\"test_data_75MB.csv\", chunksize=1000000) \n",
    "chunk_list = []  \n",
    "\n",
    "for chunk in df_chunk:  \n",
    "    chunk_list.append(chunk)\n",
    "data = pd.concat(chunk_list)\n",
    "\n",
    "print(\"%s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medium data: 1.8 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144.5809988975525 seconds\n"
     ]
    }
   ],
   "source": [
    "# 1k\n",
    "start_time = time.time()\n",
    "\n",
    "df_chunk = pd.read_csv(\"test_data_1GB.csv\", chunksize=1000) \n",
    "chunk_list = []  \n",
    "\n",
    "for chunk in df_chunk:  \n",
    "    chunk_list.append(chunk)\n",
    "df_concat = pd.concat(chunk_list)\n",
    "\n",
    "print(\"%s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106.53525710105896 seconds\n"
     ]
    }
   ],
   "source": [
    "# 10k\n",
    "start_time = time.time()\n",
    "\n",
    "df_chunk = pd.read_csv(\"test_data_1GB.csv\", chunksize=10000) \n",
    "chunk_list = []  \n",
    "\n",
    "for chunk in df_chunk:  \n",
    "    chunk_list.append(chunk)\n",
    "df_concat = pd.concat(chunk_list)\n",
    "\n",
    "print(\"%s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.17744207382202 seconds\n"
     ]
    }
   ],
   "source": [
    "# 100k\n",
    "start_time = time.time()\n",
    "\n",
    "df_chunk = pd.read_csv(\"test_data_1GB.csv\", chunksize=100000) \n",
    "chunk_list = []  \n",
    "\n",
    "for chunk in df_chunk:  \n",
    "    chunk_list.append(chunk)\n",
    "df_concat = pd.concat(chunk_list)\n",
    "\n",
    "print(\"%s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108.55591487884521 seconds\n"
     ]
    }
   ],
   "source": [
    "# 1M\n",
    "start_time = time.time()\n",
    "\n",
    "df_chunk = pd.read_csv(\"test_data_1GB.csv\", chunksize=1000000) \n",
    "chunk_list = []  \n",
    "\n",
    "for chunk in df_chunk:  \n",
    "    chunk_list.append(chunk)\n",
    "df_concat = pd.concat(chunk_list)\n",
    "\n",
    "print(\"%s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115.60315918922424 seconds\n"
     ]
    }
   ],
   "source": [
    "# 1M\n",
    "start_time = time.time()\n",
    "\n",
    "df_chunk = pd.read_csv(\"test_data_1GB.csv\", chunksize=5000000) \n",
    "chunk_list = []  \n",
    "\n",
    "for chunk in df_chunk:  \n",
    "    chunk_list.append(chunk)\n",
    "df_concat = pd.concat(chunk_list)\n",
    "\n",
    "print(\"%s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take away:\n",
    "Experiment 10k~1M and you'll likely find a optimal scale by trying within this range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**75 MB, 1M rows**\n",
    "- 1k chunsize -> 4.59s\n",
    "- 10k chunsize -> 1.97s\n",
    "- **100k chunsize -> 1.42s** `winner`\n",
    "- 1M chunsize -> 1.44s\n",
    "\n",
    "**1.8 GB, 10M rows**\n",
    "- 1k chunsize -> 144.58s\n",
    "- 10k chunsize -> 106.53s\n",
    "- **100k chunsize -> 100.17s** `winner`\n",
    "- 1M chunsize -> 115.60s\n",
    "\n",
    "Take-away:\n",
    "- There's an optimal level of chunsize we can pick based on the size of each row of our dataframe;\n",
    "- Picking a larger than optimal chunsize is more effiicent than picking a smaller one;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "data_analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
